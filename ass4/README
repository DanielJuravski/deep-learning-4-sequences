This code implements the "Natural Language Inference" task, based on the Stanford SNLI dataset.
The model is: "A Decomposable Attention Model for Natural Language Inference" (https://arxiv.org/pdf/1606.01933v1.pdf)

Requested programs:
- python2.7
- unzip (for get_data.sh)
- wget (for get_data.sh)

Requested python modules:
- dynet
- dynet_config
- numpy
- random
- datetime
- sys
- matplotlib.pyplot
- nltk
- sklearn

The code uses 3 external data sets:
1. SNLI data set - version:1.0 format:'json'.
2. GloVe data set - version:'glove.42B.300d' format:'txt'.
3. NLTK data - attached (no need to do anything).


For getting the data:
$ get_data.sh


For running the model:
    $ python snli.py <train_data> <dev_data> <test_data> <glove_data> <nltk_data>
Where:
    <train_data> is "snli_1.0_train.jsonl" file.
    <dev_data> is "snli_1.0_dev.jsonl" file.
    <test_data> is "snli_1.0_test.jsonl" file.
    <glove_data> is "glove.42B.300d.txt" file.
    <nltk_data> is "nltk_data/" dir.
For example:
    $ python snli.py snli_1.0_train.jsonl snli_1.0_dev.jsonl snli_1.0_test.jsonl glove.42B.300d.txt nltk_data/
